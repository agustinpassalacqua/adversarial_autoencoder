{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ADVERSARIAL AUTOENCODER PARA CLASIFICACIÓN SEMI SUPERVISADA"
      ],
      "metadata": {
        "id": "yuvjGlVF4-iX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un problema de clasificacion multiclase de imagenes puede resultar una tarea desafiante si no se cuenta con muchos datos etiquetados. En el paper \"Adversarial Autoencoders\" de 2016 se propone una solución. Los Adversarial Autoencoders (AAEs) ofrecen una alternativa eficiente para realizar inferencia variacional, integrando un autoencoder clásico con una componente adversarial inspirada en las Generative Adversarial Networks (GANs).\n",
        "Algunas aplicaciones son la clasificación semi-supervisada, análisis de representación y tareas de generación.\n",
        "En este trabajo nos enfocamos en su aplicación sobre el conjunto de datos MNIST y exploramos sus capacidades para aprendizaje estructurado con datos parcialmente etiquetados."
      ],
      "metadata": {
        "id": "ACua8YJr5HmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PAQUETES\n"
      ],
      "metadata": {
        "id": "HGLl4AH3tKJl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TPPBbn4s9pf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import numpy as np\n",
        "from torch.optim.lr_scheduler import MultiStepLR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONFIGURACION Y DATOS\n"
      ],
      "metadata": {
        "id": "H5xXOIMutP1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 100\n",
        "z_dim = 10  #cant de \"estilos\"\n",
        "y_dim = 10  # cant de clases (etiquetasreales del mnist)\n",
        "h_dim = 1000  # tamaño de capas ocultas\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "0QIkJbpbty0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usamos el dataset mnist\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "mnist_train = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "mnist_test = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# Hacemos el dataset para todo lo que es semi supervizado\n",
        "np.random.seed(202506)\n",
        "labels_porclase = 20\n",
        "indices = []\n",
        "for c in range(10):\n",
        "    idx = np.where(np.array(mnist_train.targets) == c)[0]\n",
        "    indices.extend(np.random.choice(idx, labels_porclase, replace=False))\n",
        "\n",
        "subconj_etiquetado = Subset(mnist_train, indices)\n",
        "idx_sin_etiqueta = list(set(range(len(mnist_train))) - set(indices))\n",
        "subjconj_sin_etiqueta = Subset(mnist_train, idx_sin_etiqueta)\n",
        "\n",
        "labeled_loader = DataLoader(subconj_etiquetado, batch_size=batch_size, shuffle=True)\n",
        "unlabeled_loader = DataLoader(subjconj_sin_etiqueta, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "62OWgMbzt6UQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# esto es una prueba aparte\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# ---------------------- PARÁMETROS ----------------------\n",
        "modo = \"por_clase\"  # \"por_clase\" o \"porcentaje\"\n",
        "labels_por_clase = 100  # solo se usa si modo == \"por_clase\"\n",
        "porcentaje_etiquetado = 0.05  # solo se usa si modo == \"porcentaje\"\n",
        "batch_size = 64\n",
        "semilla = 202506\n",
        "\n",
        "# ---------------------- CARGA DEL DATASET ----------------------\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "mnist_train = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "mnist_test = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "# ---------------------- CREACIÓN SEMISUPERVISADA ----------------------\n",
        "np.random.seed(semilla)\n",
        "targets = np.array(mnist_train.targets)\n",
        "\n",
        "if modo == \"por_clase\":\n",
        "    indices_etiquetados = []\n",
        "    for c in range(10):\n",
        "        idx_clase = np.where(targets == c)[0]\n",
        "        indices_etiquetados.extend(np.random.choice(idx_clase, labels_por_clase, replace=False))\n",
        "\n",
        "elif modo == \"porcentaje\":\n",
        "    total_etiquetados = int(len(mnist_train) * porcentaje_etiquetado)\n",
        "    indices_etiquetados = np.random.choice(len(mnist_train), total_etiquetados, replace=False).tolist()\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"El modo debe ser 'por_clase' o 'porcentaje'.\")\n",
        "\n",
        "indices_sin_etiqueta = list(set(range(len(mnist_train))) - set(indices_etiquetados))\n",
        "\n",
        "# ---------------------- CONJUNTOS DE DATOS ----------------------\n",
        "subconj_etiquetado = Subset(mnist_train, indices_etiquetados)\n",
        "subconj_sin_etiqueta = Subset(mnist_train, indices_sin_etiqueta)\n",
        "\n",
        "labeled_loader = DataLoader(subconj_etiquetado, batch_size=batch_size, shuffle=True)\n",
        "unlabeled_loader = DataLoader(subconj_sin_etiqueta, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "id": "vweZx3GfYV5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODELO"
      ],
      "metadata": {
        "id": "seng3JYTtb1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consideremos la siguiente arquitectura, donde el encoder genera dos vectores: y, el vector que representa las clases, y z, el vector que representa diferentes estilos de escritura. El decoder es entrenado para reconstruir la imagen, mientras que el encoder se entrena para generar buenas representaciones del espacio latente. Un discriminador Dy fuerza a que el vector y sea categórico, mientras que un discriminador Dz fuerza a que el vector z siga una distribución gaussiana con desviación estándar 1."
      ],
      "metadata": {
        "id": "ZdQf5oeWKfJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ENCODER"
      ],
      "metadata": {
        "id": "eRx6hVsBvF3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(784, h_dim), nn.ReLU(),\n",
        "           nn.Linear(h_dim, h_dim), nn.ReLU()\n",
        "        )\n",
        "        self.fc_z = nn.Linear(h_dim, z_dim)         # vector del \"estilo\" (este se va aprendiendo solo)\n",
        "        self.fc_y = nn.Linear(h_dim, y_dim)         # Clase\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        h = self.shared(x)\n",
        "        z = self.fc_z(h)\n",
        "        y = F.softmax(self.fc_y(h), dim=1)\n",
        "        return z, y\n"
      ],
      "metadata": {
        "id": "pkv85MUuvU_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DECODER\n",
        "\n"
      ],
      "metadata": {
        "id": "l5tSoLwdvIBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(z_dim + y_dim, h_dim), nn.ReLU(),\n",
        "            nn.Linear(h_dim, h_dim), nn.ReLU(),\n",
        "            nn.Linear(h_dim, 784), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, z, y):\n",
        "        x_hat = self.fc(torch.cat([z, y], dim=1))\n",
        "        return x_hat.view(-1, 1, 28, 28)"
      ],
      "metadata": {
        "id": "DRhEQUP-wz8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PROPUESTA ALTERNATIVA PARA ENCODER Y DECODER\n"
      ],
      "metadata": {
        "id": "UYCdx35uVBBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consideramos que 1000 neuronas en la shidden layers puede ser un numero muy alto para aprender solo 10 clases y que la red sea propensa a sobreajustar. Proponemos una arquitectura alternativa mas relacionada a un autoencoder clasico con capas que van bajando progresivamente la dimensionalidad en el encoder y aumentandola en el decoder."
      ],
      "metadata": {
        "id": "xUPAYlbqK_Qz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.Linear(784, 500), nn.ReLU(),\n",
        "            nn.Linear(500, 300), nn.ReLU()\n",
        "        )\n",
        "        self.fc_z = nn.Linear(300, z_dim)\n",
        "        self.fc_y = nn.Linear(300, y_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        h = self.shared(x)\n",
        "        z = self.fc_z(h)\n",
        "        y = F.softmax(self.fc_y(h), dim=1)\n",
        "        return z, y\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(z_dim + y_dim, 300), nn.ReLU(),\n",
        "            nn.Linear(300, 500), nn.ReLU(),\n",
        "            nn.Linear(500, 784), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, z, y):\n",
        "        return self.net(torch.cat([z, y], dim=1)).view(-1, 1, 28, 28)"
      ],
      "metadata": {
        "id": "ZrpveEt4U-Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DISCRIMINADOR Z E Y"
      ],
      "metadata": {
        "id": "X_ZSlY4avKVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiscriminadorZ(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(z_dim, h_dim), nn.ReLU(),\n",
        "            nn.Linear(h_dim, 1), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z)\n",
        "\n",
        "class DiscriminadorY(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(y_dim, h_dim), nn.ReLU(),\n",
        "            nn.Linear(h_dim, 1), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, y):\n",
        "        return self.net(y)"
      ],
      "metadata": {
        "id": "Ym9rr9h1vTBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#INICIALIZACION\n"
      ],
      "metadata": {
        "id": "Z1BfayOmtb4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estudiamos hiperparametros y decidimos realizar algunas modificaciones con respecto al paper. Vemos que la red tiene muy buenos resultados luego de 30 epochs, asi que vamos ajustando el elarning rate de forma acorde."
      ],
      "metadata": {
        "id": "-aHahGn2Lqc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder().to(device)\n",
        "decoder = Decoder().to(device)\n",
        "dz = DiscriminadorZ().to(device)\n",
        "dy = DiscriminadorY().to(device)\n",
        "\n",
        "recon_optimizador = optim.SGD(list(encoder.parameters()) + list(decoder.parameters()), lr=0.01, momentum=0.9)\n",
        "dz_optimizador = optim.SGD(dz.parameters(), lr=0.1, momentum=0.1)\n",
        "dy_optimizador = optim.SGD(dy.parameters(), lr=0.1, momentum=0.5)\n",
        "class_optimizador = optim.SGD(encoder.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "\n",
        "# learning rates según el paper\n",
        "recon_scheduler = MultiStepLR(recon_optimizador, milestones=[30, 60, 90, 120], gamma=0.1)\n",
        "class_scheduler = MultiStepLR(class_optimizador, milestones=[30, 60, 90, 120], gamma=0.1)\n",
        "dz_scheduler = MultiStepLR(dz_optimizador, milestones=[30, 60, 90, 120], gamma=0.1)\n",
        "dy_scheduler = MultiStepLR(dy_optimizador, milestones=[30, 60, 90, 120], gamma=0.1)\n",
        "\n",
        "# (el momentum esta definido con esos valores para copiar lo del paper)"
      ],
      "metadata": {
        "id": "4knkiz_AxXLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ENTRENAMIENTO"
      ],
      "metadata": {
        "id": "bcrYN4nUtoDp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos por 150 epochs y ajustamos la loss del discriminador z y de la clasificacion para mejorar el rendimiento que buscamos."
      ],
      "metadata": {
        "id": "M5vSqwJIMCm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(2500):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    for (ul_x, _), (l_x, l_y) in zip(unlabeled_loader, labeled_loader):\n",
        "        ul_x = ul_x.to(device)\n",
        "        ul_x = ul_x + 0.3 * torch.randn_like(ul_x)\n",
        "        l_x = l_x.to(device)\n",
        "        l_y = l_y.to(device)\n",
        "\n",
        "        #Reconstruccion\n",
        "        z, y = encoder(ul_x)\n",
        "        x_hat = decoder(z, y)\n",
        "        loss_recon = F.mse_loss(x_hat, ul_x)\n",
        "        recon_optimizador.zero_grad()\n",
        "        loss_recon.backward()\n",
        "        recon_optimizador.step()\n",
        "\n",
        "\n",
        "\n",
        "        #Discriminador Z\n",
        "        z_real = torch.randn(ul_x.size(0), z_dim).to(device)\n",
        "        z_fake, _ = encoder(ul_x)\n",
        "        dz_real = dz(z_real)\n",
        "        dz_fake = dz(z_fake.detach())\n",
        "        loss_dz = -torch.mean(torch.log(dz_real + 1e-8) + torch.log(1 - dz_fake + 1e-8))\n",
        "        loss_dz = 2 * loss_dz\n",
        "        dz_optimizador.zero_grad()\n",
        "        loss_dz.backward()\n",
        "        dz_optimizador.step()\n",
        "\n",
        "        # Discriminador Y\n",
        "        y_real = F.one_hot(torch.randint(0, y_dim, (ul_x.size(0),)), num_classes=y_dim).float().to(device)\n",
        "        _, y_fake = encoder(ul_x)\n",
        "        dy_real = dy(y_real)\n",
        "        dy_fake = dy(y_fake.detach())\n",
        "        loss_dy = -torch.mean(torch.log(dy_real + 1e-8) + torch.log(1 - dy_fake + 1e-8))\n",
        "        dy_optimizador.zero_grad()\n",
        "        loss_dy.backward()\n",
        "        dy_optimizador.step()\n",
        "\n",
        "        # Clasificacion (supervisada)\n",
        "        _, y_pred = encoder(l_x)\n",
        "        loss_cls = 2.0 * F.cross_entropy(y_pred, l_y)\n",
        "        class_optimizador.zero_grad()\n",
        "        loss_cls.backward()\n",
        "        class_optimizador.step()\n",
        "\n",
        "    print(f\"Epoch {epoch} | Recon: {loss_recon.item():.4f} | Dz: {loss_dz.item()/2:.4f} | Dy: {loss_dy.item():.4f} | Cls: {loss_cls.item()/2:.4f}\")\n",
        "    if epoch % 10 == 0:\n",
        "        encoder.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for x_test, y_test in test_loader:\n",
        "                x_test, y_test = x_test.to(device), y_test.to(device)\n",
        "                _, y_pred = encoder(x_test)\n",
        "                pred_labels = torch.argmax(y_pred, dim=1)\n",
        "                correct += (pred_labels == y_test).sum().item()\n",
        "                total += y_test.size(0)\n",
        "        acc = 100 * correct / total\n",
        "        print(f\"Test Accuracy: {acc:.2f}%\")\n",
        "    recon_scheduler.step()\n",
        "    class_scheduler.step()\n",
        "    dz_scheduler.step()\n",
        "    dy_scheduler.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "xiQBCE2gxu8N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8152da31-064a-40ae-a424-aa4dbe769cf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 | Recon: 0.3222 | Dz: 0.1479 | Dy: 0.7786 | Cls: 2.2934\n",
            "Test Accuracy: 38.12%\n",
            "Epoch 1 | Recon: 0.3244 | Dz: 0.0812 | Dy: 0.4719 | Cls: 2.2111\n",
            "Epoch 2 | Recon: 0.3228 | Dz: 0.0771 | Dy: 1.0719 | Cls: 1.7873\n",
            "Epoch 3 | Recon: 0.3188 | Dz: 0.0613 | Dy: 1.1415 | Cls: 1.8353\n",
            "Epoch 4 | Recon: 0.3181 | Dz: 0.0567 | Dy: 1.0689 | Cls: 1.7291\n",
            "Epoch 5 | Recon: 0.3183 | Dz: 0.0526 | Dy: 1.0735 | Cls: 1.7489\n",
            "Epoch 6 | Recon: 0.3191 | Dz: 0.0387 | Dy: 1.0748 | Cls: 1.8755\n",
            "Epoch 7 | Recon: 0.3166 | Dz: 0.0194 | Dy: 1.1496 | Cls: 1.7855\n",
            "Epoch 8 | Recon: 0.3162 | Dz: 0.0345 | Dy: 1.1186 | Cls: 1.8840\n",
            "Epoch 9 | Recon: 0.3120 | Dz: 0.0249 | Dy: 1.0219 | Cls: 1.7584\n",
            "Epoch 10 | Recon: 0.3139 | Dz: 0.0245 | Dy: 1.0230 | Cls: 1.7308\n",
            "Test Accuracy: 66.36%\n",
            "Epoch 11 | Recon: 0.3117 | Dz: 0.0238 | Dy: 1.0810 | Cls: 1.7805\n",
            "Epoch 12 | Recon: 0.3037 | Dz: 0.0491 | Dy: 1.1095 | Cls: 1.8794\n",
            "Epoch 13 | Recon: 0.2962 | Dz: 0.0042 | Dy: 1.1197 | Cls: 1.8042\n",
            "Epoch 14 | Recon: 0.2881 | Dz: 0.0036 | Dy: 1.0417 | Cls: 1.6736\n",
            "Epoch 15 | Recon: 0.2765 | Dz: 0.0030 | Dy: 1.1152 | Cls: 1.6892\n",
            "Epoch 16 | Recon: 0.2473 | Dz: 0.0194 | Dy: 1.2075 | Cls: 1.6968\n",
            "Epoch 17 | Recon: 0.2151 | Dz: 0.0075 | Dy: 1.2195 | Cls: 1.6058\n",
            "Epoch 18 | Recon: 0.1984 | Dz: 0.0022 | Dy: 1.2983 | Cls: 1.5852\n",
            "Epoch 19 | Recon: 0.1778 | Dz: 0.0048 | Dy: 1.1788 | Cls: 1.7112\n",
            "Epoch 20 | Recon: 0.1656 | Dz: 0.0022 | Dy: 1.2806 | Cls: 1.5729\n",
            "Test Accuracy: 80.10%\n",
            "Epoch 21 | Recon: 0.1707 | Dz: 0.0005 | Dy: 1.1773 | Cls: 1.5369\n",
            "Epoch 22 | Recon: 0.1641 | Dz: 0.0038 | Dy: 1.2277 | Cls: 1.5254\n",
            "Epoch 23 | Recon: 0.1614 | Dz: 0.0004 | Dy: 1.2163 | Cls: 1.6130\n",
            "Epoch 24 | Recon: 0.1592 | Dz: 0.0007 | Dy: 1.1889 | Cls: 1.5853\n",
            "Epoch 25 | Recon: 0.1626 | Dz: 0.0003 | Dy: 1.1946 | Cls: 1.5842\n",
            "Epoch 26 | Recon: 0.1634 | Dz: 0.0002 | Dy: 1.2253 | Cls: 1.6067\n",
            "Epoch 27 | Recon: 0.1616 | Dz: 0.0014 | Dy: 1.2303 | Cls: 1.5850\n",
            "Epoch 28 | Recon: 0.1614 | Dz: 0.0005 | Dy: 1.2018 | Cls: 1.5837\n",
            "Epoch 29 | Recon: 0.1650 | Dz: 0.0002 | Dy: 1.2917 | Cls: 1.5592\n",
            "Epoch 30 | Recon: 0.1608 | Dz: 0.0023 | Dy: 1.1981 | Cls: 1.5834\n",
            "Test Accuracy: 81.30%\n",
            "Epoch 31 | Recon: 0.1598 | Dz: 0.0003 | Dy: 1.2307 | Cls: 1.5833\n",
            "Epoch 32 | Recon: 0.1597 | Dz: 0.0003 | Dy: 1.2533 | Cls: 1.5096\n",
            "Epoch 33 | Recon: 0.1592 | Dz: 0.0005 | Dy: 1.2678 | Cls: 1.5829\n",
            "Epoch 34 | Recon: 0.1626 | Dz: 0.0358 | Dy: 1.2227 | Cls: 1.5349\n",
            "Epoch 35 | Recon: 0.1618 | Dz: 0.0003 | Dy: 1.1770 | Cls: 1.5365\n",
            "Epoch 36 | Recon: 0.1574 | Dz: 0.0003 | Dy: 1.2416 | Cls: 1.5094\n",
            "Epoch 37 | Recon: 0.1588 | Dz: 0.0003 | Dy: 1.2237 | Cls: 1.5831\n",
            "Epoch 38 | Recon: 0.1630 | Dz: 0.0010 | Dy: 1.2072 | Cls: 1.6549\n",
            "Epoch 39 | Recon: 0.1593 | Dz: 0.0003 | Dy: 1.1670 | Cls: 1.6571\n",
            "Epoch 40 | Recon: 0.1611 | Dz: 0.0003 | Dy: 1.2675 | Cls: 1.6326\n",
            "Test Accuracy: 81.43%\n",
            "Epoch 41 | Recon: 0.1611 | Dz: 0.0066 | Dy: 1.1687 | Cls: 1.5840\n",
            "Epoch 42 | Recon: 0.1565 | Dz: 0.0003 | Dy: 1.2359 | Cls: 1.6322\n",
            "Epoch 43 | Recon: 0.1604 | Dz: 0.0008 | Dy: 1.1955 | Cls: 1.5348\n",
            "Epoch 44 | Recon: 0.1610 | Dz: 0.0005 | Dy: 1.2367 | Cls: 1.6815\n",
            "Epoch 45 | Recon: 0.1587 | Dz: 0.0005 | Dy: 1.2717 | Cls: 1.4857\n",
            "Epoch 46 | Recon: 0.1613 | Dz: 0.0003 | Dy: 1.2385 | Cls: 1.5107\n",
            "Epoch 47 | Recon: 0.1553 | Dz: 0.0003 | Dy: 1.0903 | Cls: 1.5835\n",
            "Epoch 48 | Recon: 0.1610 | Dz: 0.0012 | Dy: 1.2353 | Cls: 1.5580\n",
            "Epoch 49 | Recon: 0.1590 | Dz: 0.0003 | Dy: 1.1995 | Cls: 1.5592\n",
            "Epoch 50 | Recon: 0.1609 | Dz: 0.0122 | Dy: 1.2787 | Cls: 1.6065\n",
            "Test Accuracy: 81.47%\n",
            "Epoch 51 | Recon: 0.1590 | Dz: 0.0003 | Dy: 1.2546 | Cls: 1.5584\n",
            "Epoch 52 | Recon: 0.1625 | Dz: 0.0002 | Dy: 1.2504 | Cls: 1.7301\n",
            "Epoch 53 | Recon: 0.1658 | Dz: 0.0072 | Dy: 1.2408 | Cls: 1.5095\n",
            "Epoch 54 | Recon: 0.1612 | Dz: 0.0002 | Dy: 1.2253 | Cls: 1.6069\n",
            "Epoch 55 | Recon: 0.1590 | Dz: 0.0001 | Dy: 1.2174 | Cls: 1.5822\n",
            "Epoch 56 | Recon: 0.1620 | Dz: 0.0058 | Dy: 1.2556 | Cls: 1.5575\n",
            "Epoch 57 | Recon: 0.1607 | Dz: 0.0002 | Dy: 1.2305 | Cls: 1.5584\n",
            "Epoch 58 | Recon: 0.1580 | Dz: 0.0011 | Dy: 1.2967 | Cls: 1.6313\n",
            "Epoch 59 | Recon: 0.1606 | Dz: 0.0011 | Dy: 1.2337 | Cls: 1.5837\n",
            "Epoch 60 | Recon: 0.1612 | Dz: 0.0006 | Dy: 1.1910 | Cls: 1.5820\n",
            "Test Accuracy: 81.42%\n",
            "Epoch 61 | Recon: 0.1616 | Dz: 0.0002 | Dy: 1.3000 | Cls: 1.6080\n",
            "Epoch 62 | Recon: 0.1640 | Dz: 0.0008 | Dy: 1.2228 | Cls: 1.5093\n",
            "Epoch 63 | Recon: 0.1646 | Dz: 0.0005 | Dy: 1.1835 | Cls: 1.6080\n",
            "Epoch 64 | Recon: 0.1591 | Dz: 0.0003 | Dy: 1.2365 | Cls: 1.6094\n",
            "Epoch 65 | Recon: 0.1556 | Dz: 0.0016 | Dy: 1.2203 | Cls: 1.5586\n",
            "Epoch 66 | Recon: 0.1624 | Dz: 0.0002 | Dy: 1.2348 | Cls: 1.5578\n",
            "Epoch 67 | Recon: 0.1578 | Dz: 0.0013 | Dy: 1.2315 | Cls: 1.5826\n",
            "Epoch 68 | Recon: 0.1587 | Dz: 0.0005 | Dy: 1.2236 | Cls: 1.5593\n",
            "Epoch 69 | Recon: 0.1630 | Dz: 0.0011 | Dy: 1.2402 | Cls: 1.5109\n",
            "Epoch 70 | Recon: 0.1590 | Dz: 0.0002 | Dy: 1.2151 | Cls: 1.5584\n",
            "Test Accuracy: 81.42%\n",
            "Epoch 71 | Recon: 0.1591 | Dz: 0.0005 | Dy: 1.2450 | Cls: 1.6320\n",
            "Epoch 72 | Recon: 0.1572 | Dz: 0.0012 | Dy: 1.2328 | Cls: 1.6789\n",
            "Epoch 73 | Recon: 0.1579 | Dz: 0.0006 | Dy: 1.2301 | Cls: 1.5575\n",
            "Epoch 74 | Recon: 0.1643 | Dz: 0.0004 | Dy: 1.2457 | Cls: 1.5829\n",
            "Epoch 75 | Recon: 0.1569 | Dz: 0.0004 | Dy: 1.2584 | Cls: 1.5579\n",
            "Epoch 76 | Recon: 0.1636 | Dz: 0.0002 | Dy: 1.2665 | Cls: 1.5840\n",
            "Epoch 77 | Recon: 0.1581 | Dz: 0.0007 | Dy: 1.2414 | Cls: 1.5816\n",
            "Epoch 78 | Recon: 0.1614 | Dz: 0.0002 | Dy: 1.2233 | Cls: 1.5828\n",
            "Epoch 79 | Recon: 0.1622 | Dz: 0.0005 | Dy: 1.2070 | Cls: 1.5591\n",
            "Epoch 80 | Recon: 0.1591 | Dz: 0.0005 | Dy: 1.2182 | Cls: 1.5811\n",
            "Test Accuracy: 81.42%\n",
            "Epoch 81 | Recon: 0.1604 | Dz: 0.0010 | Dy: 1.2070 | Cls: 1.5333\n",
            "Epoch 82 | Recon: 0.1609 | Dz: 0.0004 | Dy: 1.1787 | Cls: 1.6070\n",
            "Epoch 83 | Recon: 0.1623 | Dz: 0.0003 | Dy: 1.2478 | Cls: 1.5821\n",
            "Epoch 84 | Recon: 0.1599 | Dz: 0.0005 | Dy: 1.2163 | Cls: 1.5343\n",
            "Epoch 85 | Recon: 0.1640 | Dz: 0.0007 | Dy: 1.2890 | Cls: 1.5601\n",
            "Epoch 86 | Recon: 0.1620 | Dz: 0.0050 | Dy: 1.2108 | Cls: 1.5587\n",
            "Epoch 87 | Recon: 0.1595 | Dz: 0.0024 | Dy: 1.2390 | Cls: 1.5837\n",
            "Epoch 88 | Recon: 0.1588 | Dz: 0.0003 | Dy: 1.2138 | Cls: 1.6565\n",
            "Epoch 89 | Recon: 0.1623 | Dz: 0.0002 | Dy: 1.2729 | Cls: 1.5597\n",
            "Epoch 90 | Recon: 0.1643 | Dz: 0.0002 | Dy: 1.2196 | Cls: 1.5356\n",
            "Test Accuracy: 81.41%\n",
            "Epoch 91 | Recon: 0.1599 | Dz: 0.0051 | Dy: 1.2336 | Cls: 1.6310\n",
            "Epoch 92 | Recon: 0.1610 | Dz: 0.0013 | Dy: 1.2005 | Cls: 1.5099\n",
            "Epoch 93 | Recon: 0.1599 | Dz: 0.0019 | Dy: 1.1854 | Cls: 1.6071\n",
            "Epoch 94 | Recon: 0.1538 | Dz: 0.0002 | Dy: 1.1735 | Cls: 1.5344\n",
            "Epoch 95 | Recon: 0.1611 | Dz: 0.0003 | Dy: 1.2245 | Cls: 1.5584\n",
            "Epoch 96 | Recon: 0.1657 | Dz: 0.0003 | Dy: 1.2749 | Cls: 1.6316\n",
            "Epoch 97 | Recon: 0.1612 | Dz: 0.0025 | Dy: 1.2148 | Cls: 1.5345\n",
            "Epoch 98 | Recon: 0.1584 | Dz: 0.0002 | Dy: 1.2350 | Cls: 1.5336\n",
            "Epoch 99 | Recon: 0.1586 | Dz: 0.0002 | Dy: 1.2572 | Cls: 1.5576\n",
            "Epoch 100 | Recon: 0.1569 | Dz: 0.0003 | Dy: 1.2433 | Cls: 1.6562\n",
            "Test Accuracy: 81.41%\n",
            "Epoch 101 | Recon: 0.1623 | Dz: 0.0002 | Dy: 1.2003 | Cls: 1.6573\n",
            "Epoch 102 | Recon: 0.1630 | Dz: 0.0003 | Dy: 1.1788 | Cls: 1.6064\n",
            "Epoch 103 | Recon: 0.1556 | Dz: 0.0003 | Dy: 1.2158 | Cls: 1.5818\n",
            "Epoch 104 | Recon: 0.1623 | Dz: 0.0004 | Dy: 1.1956 | Cls: 1.5110\n",
            "Epoch 105 | Recon: 0.1598 | Dz: 0.0002 | Dy: 1.2320 | Cls: 1.5839\n",
            "Epoch 106 | Recon: 0.1604 | Dz: 0.0002 | Dy: 1.2050 | Cls: 1.6070\n",
            "Epoch 107 | Recon: 0.1593 | Dz: 0.0002 | Dy: 1.2037 | Cls: 1.5343\n",
            "Epoch 108 | Recon: 0.1555 | Dz: 0.0012 | Dy: 1.2148 | Cls: 1.4613\n",
            "Epoch 109 | Recon: 0.1628 | Dz: 0.0005 | Dy: 1.2279 | Cls: 1.6561\n",
            "Epoch 110 | Recon: 0.1606 | Dz: 0.0007 | Dy: 1.1523 | Cls: 1.6080\n",
            "Test Accuracy: 81.41%\n",
            "Epoch 111 | Recon: 0.1575 | Dz: 0.0003 | Dy: 1.1620 | Cls: 1.6068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLASIFICACION DE NUMERO DIBUJADO"
      ],
      "metadata": {
        "id": "Cjt8kLzW9KzJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El siguiente codigo es para probar el modelo con un numero dibujado a mano."
      ],
      "metadata": {
        "id": "sd14bGopMUzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocesamiento de imagen"
      ],
      "metadata": {
        "id": "BMIbjz9P-BaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageOps, ImageFilter, ImageEnhance\n",
        "import torchvision.transforms as T\n",
        "import torch\n",
        "\n",
        "def procesar_imagen(foto):\n",
        "    imagen = Image.open(foto).convert(\"L\")\n",
        "    imagen = ImageOps.invert(imagen)\n",
        "\n",
        "    # contraste\n",
        "    enhancer = ImageEnhance.Contrast(imagen)\n",
        "    imagen = enhancer.enhance(3)\n",
        "\n",
        "    imagen = imagen.resize((28, 28))\n",
        "\n",
        "    transform = T.Compose([\n",
        "        T.ToTensor(),\n",
        "        T.Normalize((0.1307,), (0.3081,)) #como mnist\n",
        "    ])\n",
        "    tensor = transform(imagen)\n",
        "    tensor = tensor.unsqueeze(0)\n",
        "    return tensor\n"
      ],
      "metadata": {
        "id": "ksn9v9wwSV12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clasificacion\n"
      ],
      "metadata": {
        "id": "3YeBeClPYxoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeros= [\"0.jpg\", \"1.jpeg\", \"2.jpeg\", \"3.jpeg\", \"4.jpg\", \"5.jpg\", \"6.jpg\", \"7.jpg\", \"8.jpg\", \"9.jpg\"]\n",
        "for i in numeros:\n",
        "    imagen_tensor = procesar_imagen(i)\n",
        "    z, y_pred = encoder(imagen_tensor.to(device))\n",
        "    clase = torch.argmax(y_pred, dim=1).item()\n",
        "   # print(imagen_tensor.min(), imagen_tensor.max(), imagen_tensor.mean())\n",
        "    print(f\"Número predicho por la red: {clase}\")"
      ],
      "metadata": {
        "id": "8ldDuLYuAHZq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d2260ce-27c7-47a4-d7fa-33302f308674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número predicho por la red: 0\n",
            "Número predicho por la red: 1\n",
            "Número predicho por la red: 2\n",
            "Número predicho por la red: 3\n",
            "Número predicho por la red: 4\n",
            "Número predicho por la red: 2\n",
            "Número predicho por la red: 5\n",
            "Número predicho por la red: 1\n",
            "Número predicho por la red: 5\n",
            "Número predicho por la red: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DISCUSION"
      ],
      "metadata": {
        "id": "1E7BGxDXM-AV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego de 150 épocas, se logró una precisión del 80% sobre un conjunto de evaluación.\n",
        "\n",
        "La red fue probada con dígitos dibujados a mano, alcanzando una precisión cercana al 50%. Mostró buen desempeño en la detección de los dígitos del 0 al 4, pero presentó dificultades con el 5, 6, 7 y 8.\n",
        "\n",
        "No se alcanzó un equilibrio de Nash entre el encoder y los discriminadores: en todos los casos, el encoder aprendió a generar vectores Y representativos, pero rara vez logró producir vectores Z lo suficientemente similares a los reales como para engañar al discriminador."
      ],
      "metadata": {
        "id": "yT9O6DdtNAMM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONCLUSIONES"
      ],
      "metadata": {
        "id": "VkV0xNBJvYQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este trabajo se desarrolló un Autoencoder Adversarial (AAE) para clasificación semi-supervisada del dataset MNIST, utilizando únicamente 200 ejemplos etiquetados. A pesar de la escasez de datos supervisados, se logró una precisión del 80% sobre el conjunto de testing, lo cual valida el potencial de los modelos generativos adversariales para tareas con bajo acceso a etiquetas.\n",
        "\n",
        "El diseño del AAE se basó en dividir la representación latente en dos componentes:\n",
        "z: codifica el estilo(información no supervisada)\n",
        "y: codifica la clase(etiqueta supervisada)\n",
        "\n",
        "Durante el entrenamiento, el encoder aprendió efectivamente a generar representaciones y útiles para la clasificación. Sin embargo, la parte no supervisada z no logró aproximar adecuadamente la distribución gaussiana deseada, y el discriminador correspondiente detectó fácilmente que los vectores z eran generados. Esto sugiere que no se alcanzó un equilibriopleno, y que el espacio latente no fue regularizado de la mejor manera.\n",
        "\n",
        "A nivel teórico, esto se puede interpretar como una falla en lograr un equilibrio de Nash Nash entre el encoder y los discriminadores. En un AAE bien entrenado, el encoder debería ser capaz de engañar al discriminador , haciendo que sus salidas sean indistinguibles de muestras reales. Si esto no ocurre, el modelo puede sobreajustarse al objetivo de clasificación y perder la riqueza del espacio latente, lo cual afecta la generalización, en especial cuando se evalúa con datos fuera de distribución, como dígitos dibujados a mano.\n",
        "\n",
        "Esta experiencia pone de manifiesto tanto el potencial como las dificultades prácticas de entrenar modelos adversariales en entornos semi-supervisados. La arquitectura AAE es conceptualmente elegante, pero altamente sensible al equilibrio entre sus componentes. Abordar este desafío abre puertas no solo a mejores clasificadores, sino también a modelos generativos que comprendan de forma más profunda la estructura de los datos."
      ],
      "metadata": {
        "id": "6WJulOJnviaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Líneas de trabajo futuras y posibles mejoras a la red:\n",
        "\n",
        "Agregar capas convolucionales: Las redes convolucionales son especialmente efectivas en visión computacional, ya que aprovechan la estructura espacial de las imágenes. Reemplazar las capas densas del encoder y decoder por capas convolucionales podría mejorar la extracción de características locales y reducir la sensibilidad a trazos y estilos personales.\n",
        "\n",
        "Preentrenar el autoencoder sin adversarialidad: Entrenar primero el autoencoder solo con la reconstrucción podría permitir que el modelo aprenda una representación inicial más estable, y luego introducir el entrenamiento adversarial de manera progresiva.\n",
        "\n",
        "Aplicar data augmentation: Esto es especialmente relevante si se pretende usar el modelo con imágenes dibujadas por humanos. Aumentar la variedad durante el entrenamiento (rotaciones, traslaciones, ruido) puede robustecer la red frente a variaciones de estilo\n",
        "\n",
        "Mejorar la estrategia de selección de datos etiquetados: En lugar de elegir al azar, pero podría explorarse una selección informada que garantice una mejor cobertura de las distintas clases y estilos.\n",
        "\n"
      ],
      "metadata": {
        "id": "l2U0u0D5zn12"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REFERENCIAS"
      ],
      "metadata": {
        "id": "IXJLBz_RHboo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., & Frey, B. (2016, mayo 25). Adversarial autoencoders. arXiv.\n",
        "https://arxiv.org/abs/1511.05644\n"
      ],
      "metadata": {
        "id": "PmI4az_pHb6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar los pesos del encoder\n",
        "torch.save(encoder.state_dict(), \"encoder_final.pth\")\n",
        "torch.save(decoder.state_dict(), \"decoder_final.pth\")\n",
        "torch.save(dz.state_dict(), \"dz_final.pth\")\n",
        "torch.save(dy.state_dict(), \"dy_final.pth\")\n"
      ],
      "metadata": {
        "id": "2CTqTgwV4IyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definí la arquitectura tal como en el entrenamiento\n",
        "encoder = Encoder().to(device)\n",
        "\n",
        "# Cargar pesos entrenados\n",
        "encoder.load_state_dict(torch.load(\"encoder_final.pth\"))\n",
        "encoder.eval()\n"
      ],
      "metadata": {
        "id": "HVsscUbi4Vfy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}